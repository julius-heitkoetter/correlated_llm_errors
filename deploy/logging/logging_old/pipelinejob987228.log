Starting Setup
Setup Complete
Starting Pipeline Run
Supervisor model names :  ['LlamaLLM', 'LlamaLLM', 'LlamaLLM']
Supervisor model configs:  ['llama_70b_base_config', 'llama_13b_base_config', 'llama_70b_base_config']
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.99s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.03s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.66s/it]
Traceback (most recent call last):
  File "/data/julius_heitkoetter/correlated_llm_errors/bin/dataset_pipeline_multiple_supervisors.py", line 145, in <module>
    run_pipeline_on_dataset(args.dataset_name, args.category, args.save_location, args.deceiver_model_name, args.deceiver_config_name, args.supervisor_model_names, args.supervisor_config_names, args.num_samples)
  File "/data/julius_heitkoetter/correlated_llm_errors/bin/dataset_pipeline_multiple_supervisors.py", line 94, in run_pipeline_on_dataset
    supervisor_llm = MODEL_MAPPING[supervisor_model_name](**CONFIG_MAPPING[supervisor_config_name])
  File "/data/julius_heitkoetter/correlated_llm_errors/lib/models.py", line 313, in __init__
    base_model = LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path=self.base_model, local_files_only= False, load_in_8bit=self.quantization, device_map='auto', torch_dtype = torch.float16)
  File "/data/julius_heitkoetter/miniconda3/envs/correlated_errors/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3114, in from_pretrained
    raise ValueError(
ValueError: 
                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit
                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping
                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom
                        `device_map` to `from_pretrained`. Check
                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu
                        for more details.
                        
