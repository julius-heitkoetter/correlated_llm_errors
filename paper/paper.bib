@article{hendrycks2021test,
    title={Measuring Massive Multitask Language Understanding},
    author={Dan Hendrycks and Collin Burns and Steven Basart et al.},
    journal={Proceedings of the International Conference on Learning Representations (ICLR)},
    year={2021}
}

@article{hendrycks2021ethics,
    title={Aligning AI With Shared Human Values},
    author={Dan Hendrycks and Collin Burns and Steven Basart et al.},
    journal={Proceedings of the International Conference on Learning Representations (ICLR)},
    year={2021}
}

@misc{Bai2022constitutionalAI,
Author = {Yuntao Bai and Saurav Kadavath and Sandipan Kundu et al.},
Title = {Constitutional AI: Harmlessness from AI Feedback},
Year = {2022},
Eprint = {arXiv:2212.08073},
}

@misc{bowman2022oversight,
    title={Measuring Progress on Scalable Oversight for Large Language Models},
    author={Bowman et al.},
    year={2022}
}

@inproceedings{biderman2023pythia,
    author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin et al.},
    title = {Pythia: A Suite for Analyzing Large Language Models across Training and Scaling},
    year = {2023},
    publisher = {JMLR.org},
    abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend Pythia to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.},
    booktitle = {Proceedings of the 40th International Conference on Machine Learning},
    articleno = {102},
    numpages = {34},
    location = {Honolulu, Hawaii, USA},
    series = {ICML'23}
}

@misc{Du2023debate,
Author = {Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum et al.},
Title = {Improving Factuality and Reasoning in Language Models through Multiagent Debate},
Year = {2023},
Eprint = {arXiv:2305.14325},
}

@misc{touvron2023llama2,
    author = {Touvron et al.},
    title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
    archivePrefix = {arXiv},
    eprint = {2307.09288},
    primaryClass = {cs.CL},
    year = {2023}
}

@misc{Shen2023survey,
Author = {Tianhao Shen and Renren Jin and Yufei Huang et al.},
Title = {Large Language Model Alignment: A Survey},
Year = {2023},
Eprint = {arXiv:2309.15025},
}

@misc{sharma2023sycophancy,
    Author = {Mrinank Sharma and Meg Tong and Tomasz Korbak et al.},
    Title = {Towards Understanding Sycophancy in Language Models},
    Year = {2023},
    Eprint = {arXiv:2310.13548},
}

@misc{belrose2023leace,
    Author = {Nora Belrose and David Schneider-Joseph and Shauli Ravfogel et al.},
    Title = {LEACE: Perfect Linear Concept Erasure in Closed Form},
    Year = {2023},
    Eprint = {arXiv:2306.03819},
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey et al.},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom et al.},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{DBLP:journals/corr/abs-2201-11903,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans et al.},
  title        = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2201.11903},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.11903},
  eprinttype    = {arXiv},
  eprint       = {2201.11903},
  timestamp    = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-11903.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% evals
@article{GuoEvaluatingLLMsSurvey,
Author = {Zishan Guo and Renren Jin and Chuang Liu et al.},
Title = {Evaluating Large Language Models: A Comprehensive Survey},
Year = {2023},
Eprint = {arXiv:2310.19736},
}

@misc{TikhonovPostTuring,
Author = {Alexey Tikhonov and Ivan P. Yamshchikov},
Title = {Post Turing: Mapping the landscape of LLM Evaluation},
Year = {2023},
Eprint = {arXiv:2311.02049},
}

@misc{ZhouEvalCheater,
Author = {Kun Zhou and Yutao Zhu and Zhipeng Chen et al.},
Title = {Don't Make Your LLM an Evaluation Benchmark Cheater},
Year = {2023},
Eprint = {arXiv:2311.01964},
}

@misc{HuangBenchmarkingResearchAgents,
Author = {Qian Huang and Jian Vora and Percy Liang et al.},
Title = {Benchmarking Large Language Models As AI Research Agents},
Year = {2023},
Eprint = {arXiv:2310.03302},
}

@misc{MialonGAIA,
Author = {Grégoire Mialon and Clémentine Fourrier and Craig Swift et al.},
Title = {GAIA: a benchmark for General AI Assistants},
Year = {2023},
Eprint = {arXiv:2311.12983},
}

@article{BigBench,
Author = {Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb et al.},
Title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
Year = {2022},
Eprint = {arXiv:2206.04615},
Howpublished = {Transactions on Machine Learning Research, May/2022,
  https://openreview.net/forum?id=uyTL5Bvosj},
}

@article{LiangHolisticEvals,
Author = {Percy Liang and Rishi Bommasani and Tony Lee et al.},
Title = {Holistic Evaluation of Language Models},
Year = {2022},
Eprint = {arXiv:2211.09110},
Howpublished = {Published in Transactions on Machine Learning Research (TMLR),
  2023},
}

@misc{AzariaLying,
Author = {Amos Azaria and Tom Mitchell},
Title = {The Internal State of an LLM Knows When It's Lying},
Year = {2023},
Eprint = {arXiv:2304.13734},
}

@misc{LinTruthQA,
Author = {Stephanie Lin and Jacob Hilton and Owain Evans},
Title = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
Year = {2021},
Eprint = {arXiv:2109.07958},
}

@article{Wei2022ChainOT,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans et al.},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.11903},
  url={https://api.semanticscholar.org/CorpusID:246411621}
}

@article{Wang2022SelfConsistencyIC,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Xuezhi Wang and Jason Wei and Dale Schuurmans et al.},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.11171},
  url={https://api.semanticscholar.org/CorpusID:247595263}
}

@misc{CobbeVerifiers,
Author = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian et al.},
Title = {Training Verifiers to Solve Math Word Problems},
Year = {2021},
Eprint = {arXiv:2110.14168},
}


@misc{HallucinationsSurvey.01219,
Author = {Yue Zhang and Yafu Li and Leyang Cui et al.},
Title = {Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
Year = {2023},
Eprint = {arXiv:2309.01219},
}


@inproceedings{perez2023discovering,
    title = "Discovering Language Model Behaviors with Model-Written Evaluations",
    author = "Perez, Ethan  and
      Ringer, Sam  and
      Lukosiute, Kamile  et al.",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.847",
    doi = "10.18653/v1/2023.findings-acl.847",
    pages = "13387--13434",
    abstract = "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100{\%} of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user{'}s preferred answer ({``}sycophancy{''}) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.",
}

@article{lee2016varianceweighted,
    Author = {Cue Hyunkyu Lee and Seungho Cook and Ji Sung Lee and Buhm Han},
    Title = {Comparison of Two Meta-Analysis Methods: Inverse-Variance-Weighted Average and Weighted Sum of Z-Scores},
    Year = {2016},
    journal={Genomics Inform},
    doi={10.5808/GI.2016.14.4.173},
}

@misc{DeceptionSurvey,
Author = {Peter S. Park and Simon Goldstein and Aidan O'Gara and Michael Chen and Dan Hendrycks},
Title = {AI Deception: A Survey of Examples, Risks, and Potential Solutions},
Year = {2023},
Eprint = {arXiv:2308.14752},
}

@article{SurveyHallucinations,
Author = {Ziwei Ji and Nayeon Lee and Rita Frieske et al.},
Title = {Survey of Hallucination in Natural Language Generation},
Year = {2022},
Eprint = {arXiv:2202.03629},
Howpublished = {ACM Computing Surveys (2022)},
Doi = {10.1145/3571730},
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick et al.},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}



@article{Cicero,
    title = {Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
    author = {Bakhtin, Anton and Brown, Noam and Dinan, Emily},
    journal = {Science},
    volume = {378},
    number = {6624},
    pages = {1067-1074},
    year = {2022},
    doi = {10.1126/science.ade9097},
    URL = {https://www.science.org/doi/abs/10.1126/science.ade9097},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.ade9097},
}
